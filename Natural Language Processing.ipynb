{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimate Guide to Understand & Implement Natural Language Processing (with codes in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "    1. Introduction to NLP\n",
    "    2. Text Preprocessing\n",
    "        2.1 Noise Removal\n",
    "        2.2 Lexicon Normalization\n",
    "            2.2.1 Lemmatization\n",
    "            2.2.2 Stemming\n",
    "        2.3 Object Standardization\n",
    "    3. Text to Features (Feature Engineering on text data)\n",
    "        3.1 Syntactical Parsing\n",
    "            3.1.1 Dependency Grammar\n",
    "            3.1.2 Part of Speech Tagging\n",
    "        3.2 Entity Parsing\n",
    "            3.2.1 Phrase Detection\n",
    "            3.2.2 Named Entity Recognition\n",
    "            3.2.3 Topic Modelling\n",
    "            3.2.4 N-Grams\n",
    "        3.3 Statistical features\n",
    "            3.3.1 TF – IDF\n",
    "            3.3.2 Frequency / Density Features\n",
    "            3.3.3 Readability Features\n",
    "        3.4 Word Embeddings\n",
    "    4.Important tasks of NLP\n",
    "        4.1 Text Classification\n",
    "        4.2 Text Matching\n",
    "            4.2.1 Levenshtein Distance\n",
    "            4.2.2 Phonetic Matching\n",
    "            4.2.3 Flexible String Matching\n",
    "        4.3 Coreference Resolution\n",
    "        4.4 Other Problems\n",
    "    5.Important NLP libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            1. Introduction to NLP\n",
    "    \n",
    "    Applications of NLP\n",
    "\n",
    "    automatic summarization\n",
    "    machine translation\n",
    "    named entity recognition\n",
    "    relationship extraction\n",
    "    sentiment analysis\n",
    "    speech recognition\n",
    "    topic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.1 Noise Removal\n",
    "    Step 1:prepare a dictionary of noisy entities, \n",
    "    Step 2:and iterate the text object by tokens (or by words), \n",
    "    Step 3:eliminating those tokens which are present in the noise dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove noisy words from a text\n",
    "\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample code to remove a regex pattern \n",
    "import re \n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\"  \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.2 Lexicon Normalization\n",
    "        \n",
    "       2.2.1 Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "       2.2.2 Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multipli\n",
      "multiply\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word =\"multiplying\"\n",
    "\n",
    "print(stem.stem(word))\n",
    "print(lem.lemmatize(word,\"v\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2.3 Object Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Retweet']\n",
      "['Retweet', 'this']\n",
      "['Retweet', 'this', 'is']\n",
      "['Retweet', 'this', 'is', 'a']\n",
      "['Retweet', 'this', 'is', 'a', 'retweeted']\n",
      "['Retweet', 'this', 'is', 'a', 'retweeted', 'tweet']\n",
      "['Retweet', 'this', 'is', 'a', 'retweeted', 'tweet', 'by']\n",
      "['Retweet', 'this', 'is', 'a', 'retweeted', 'tweet', 'by', 'Shivam']\n",
      "['Retweet', 'this', 'is', 'a', 'retweeted', 'tweet', 'by', 'Shivam', 'Bansal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by Shivam Bansal'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        \n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "        print(new_words) # for understanding the process\n",
    "    return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.Text to Features (Feature Engineering on text data)\n",
    "            3.1 Syntactic Parsing\n",
    "                3.1.1 Dependency Trees\n",
    "                3.1.2 Part of Speech Tagging (POS Tagging)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.1.2 Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'on', 'Analytics', 'Vidhya'] \n",
      "\n",
      "['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'on', 'Analytics', 'Vidhya'] \n",
      "\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens,\"\\n\")\n",
    "tokens2 = re.split(r'\\s',text)\n",
    "print(tokens2,\"\\n\")\n",
    "print (pos_tag(tokens))\n",
    "print (pos_tag(tokens2))\n",
    "# regular expression fails at some places example : John's , isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ashu6811/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'), (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "[('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'), (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")))\n",
    "\n",
    "print(pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Please', 'NOUN'), ('book', 'NOUN'), ('my', 'PRON'), ('flight', 'NOUN'), ('for', 'ADP'), ('Delhi', 'NOUN')]\n",
      "[('I', 'PRON'), ('am', 'VERB'), ('going', 'VERB'), ('to', 'PRT'), ('read', 'VERB'), ('this', 'DET'), ('book', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('flight', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "#A.Word sense disambiguation\n",
    "#book is first sentence is verb and in second sentence is noun. This isn't predicted by pos_tag\n",
    "#Lesk Algorithm comes to our rescue\n",
    "print(pos_tag(word_tokenize(\"Please book my flight for Delhi\"), tagset='universal'))\n",
    "print(pos_tag(word_tokenize(\"I am going to read this book in the flight\"),  tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2 Entity Extraction (Entities as features)\n",
    "            Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
    "            3.2.1 Named Entity Recognition (NER)\n",
    "            3.2.2 Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(34 unique tokens: ['not', 'my', 'My', 'suggest', 'of']...)\n",
      "\n",
      "matrix: [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1)], [(1, 1), (5, 1), (8, 1), (12, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(15, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)]]\n",
      "\n",
      "ldamodel: [(0, '0.063*\"to\" + 0.036*\"Sugar\" + 0.036*\"not\" + 0.036*\"but\" + 0.036*\"bad\" + 0.036*\"is\" + 0.036*\"sugar,\" + 0.036*\"likes\" + 0.036*\"have\" + 0.036*\"consume.\"'), (1, '0.029*\"driving\" + 0.029*\"sister\" + 0.029*\"My\" + 0.029*\"my\" + 0.029*\"to\" + 0.029*\"cause\" + 0.029*\"blood\" + 0.029*\"may\" + 0.029*\"suggest\" + 0.029*\"increased\"'), (2, '0.053*\"driving\" + 0.053*\"My\" + 0.053*\"sister\" + 0.053*\"my\" + 0.053*\"to\" + 0.053*\"practice.\" + 0.053*\"lot\" + 0.053*\"of\" + 0.053*\"father\" + 0.053*\"dance\"')]\n"
     ]
    }
   ],
   "source": [
    "#TOPIC MODELING\n",
    "#there is another code uploaded on github independently explaining topic modeling\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "print(dictionary)\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print(\"\\nmatrix:\",doc_term_matrix)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(\"\\nldamodel:\",ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        3.2.3 N-Grams as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sugar', 'is'],\n",
       " ['is', 'bad'],\n",
       " ['bad', 'to'],\n",
       " ['to', 'consume.'],\n",
       " ['consume.', 'My'],\n",
       " ['My', 'sister'],\n",
       " ['sister', 'likes'],\n",
       " ['likes', 'to'],\n",
       " ['to', 'have'],\n",
       " ['have', 'sugar,'],\n",
       " ['sugar,', 'but'],\n",
       " ['but', 'not'],\n",
       " ['not', 'my'],\n",
       " ['my', 'father.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "generate_ngrams(doc1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.3 Statistical Features\n",
    "        3.3.1 Term Frequency – Inverse Document Frequency (TF – IDF)\n",
    "        3.3.2 Count / Density / Readability Features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.3.1 Term Frequency – Inverse Document Frequency (TF – IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.58448290102\n",
      "  (0, 2)\t0.58448290102\n",
      "  (0, 4)\t0.444514311537\n",
      "  (0, 1)\t0.345205016865\n",
      "  (1, 1)\t0.385371627466\n",
      "  (1, 0)\t0.652490884513\n",
      "  (1, 3)\t0.652490884513\n",
      "  (2, 4)\t0.444514311537\n",
      "  (2, 1)\t0.345205016865\n",
      "  (2, 6)\t0.58448290102\n",
      "  (2, 5)\t0.58448290102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.3.2 Count / Density / Readability Features\n",
    "    Some of the features are: \n",
    "    Word Count, Sentence Count, Punctuation Counts \n",
    "    and Industry specific word counts.\n",
    "    Other types of measures include readability measures such as syllable counts,\n",
    "    smog index and flesch reading ease. \n",
    "    Refer to Textstat library to create such features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3.4 Word Embedding (text vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0332806159459\n",
      "[ -3.74489790e-03   4.30184737e-05   4.85881791e-03   3.99604253e-03\n",
      "  -3.05236247e-03  -4.72965511e-03   3.89103661e-04  -1.63321255e-03\n",
      "  -2.33896566e-03  -4.74427873e-03   4.84594749e-03  -4.05107532e-03\n",
      "   2.88611627e-04  -3.40197003e-04  -2.34870962e-03  -2.86212831e-04\n",
      "   7.35686393e-04   1.17057447e-04  -2.19545418e-05  -1.20955636e-03\n",
      "  -2.74638343e-03  -7.71940453e-04  -2.49645353e-04   3.23865097e-04\n",
      "  -6.57792029e-04  -1.18192553e-03   1.46145860e-04   1.26875460e-03\n",
      "  -2.67122436e-04  -4.41335654e-03  -3.23761581e-03   4.66149766e-03\n",
      "   2.14769319e-03  -3.45350895e-03   8.87754024e-04  -3.29132844e-03\n",
      "   1.50818343e-03  -4.91187035e-04   4.13787132e-03   2.25335360e-03\n",
      "   1.38052274e-03  -4.81582945e-03   2.37386371e-03  -4.48366132e-04\n",
      "  -4.77828924e-03   4.47241729e-03   1.20649545e-03   1.93272578e-03\n",
      "  -4.95557813e-03   1.72483316e-03  -5.15283667e-04   6.43836276e-04\n",
      "   1.86065445e-03  -2.02088244e-03  -2.03146413e-03  -8.80531152e-04\n",
      "  -3.14491056e-03  -4.02090233e-03  -3.58843221e-03  -2.08260282e-03\n",
      "   2.17705360e-03   4.80405171e-04  -2.60230061e-03   3.54306563e-03\n",
      "   4.77659656e-03   4.06109495e-03  -5.55282095e-05  -1.10756967e-03\n",
      "   3.83044058e-03   1.68387406e-03  -3.17843194e-04   3.71343433e-03\n",
      "   3.52164428e-03  -3.96628445e-03   5.55023900e-04   4.07817752e-05\n",
      "   3.46361590e-03   3.18140141e-03  -2.38449476e-03   1.84383208e-03\n",
      "   4.12466330e-03   4.59329365e-03  -4.62460564e-03   4.75210790e-03\n",
      "   1.32959773e-04   5.65418741e-04  -3.47148674e-03   2.53045885e-03\n",
      "   1.04941439e-03   3.04273656e-03  -3.16588464e-03   1.55654748e-03\n",
      "   2.21237773e-03   5.13968931e-04   1.87702302e-03   2.05443866e-04\n",
      "  -4.77757771e-03   3.89135792e-03  -3.48921842e-03  -4.56991699e-03]\n"
     ]
    }
   ],
   "source": [
    "#there is a separate word embedding code book available on github\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print (model.similarity('data', 'science'))\n",
    "\n",
    "print (model['learning']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Important tasks of NLP\n",
    "        4.1 Text Classification\n",
    "        4.2 Text Matching / Similarity\n",
    "        4.3 Coreference Resolution\n",
    "        4.4 Other NLP problems / tasks\n",
    "            4.4.1 Text Summarization\n",
    "            4.4.2 Machine Translation\n",
    "            4.4.3 Natural Language Generation and Understanding\n",
    "            4.4.4 Optical Character Recognition\n",
    "            4.4.5 Document to Information\n",
    "            \n",
    "    5. Important Libraries for NLP (python)\n",
    "        5.1 Scikit-learn: Machine learning in Python\n",
    "        5.2 Natural Language Toolkit (NLTK):The complete toolkit for all NLP techniques.\n",
    "        5.3 Pattern:A web mining module for the with tools for NLP and machine learning.\n",
    "        5.4 TextBlob : Easy to use nl p tools API, built on top of NLTK and Pattern.\n",
    "        5.5 spaCy : Industrial strength N LP with Python and Cython.\n",
    "        5.6 Gensim : Topic Modelling for Humans\n",
    "        5.7 Stanford Core NLP : NLP services and packages by Stanford NLP Group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "Class_B\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Text Classification using naive bayes \n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "training_corpus = [\n",
    "                   ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [\n",
    "                (\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    " \n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "\n",
    "print(model.accuracy(test_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       0.50      0.67      0.57         3\n",
      "    Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "avg / total       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import svm \n",
    "\n",
    "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "\n",
    "\n",
    "print (classification_report(test_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.1 Levenshtein Distance\n",
    "        The Levenshtein distance between two strings is defined as the minimum number of edits needed to transform one string into the other, with the allowable edit operations being insertion, deletion, or substitution of a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.2 Phonetic Matching\n",
    "    A Phonetic matching algorithm takes a keyword as input (person’s name, location name etc) and produces a character string that identifies a set of words that are (roughly) phonetically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b5011c427cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuzzy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msoundex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuzzy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ankit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msoundex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aunkit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'fuzzy'"
     ]
    }
   ],
   "source": [
    "from sklearn import fuzzy \n",
    "soundex = fuzzy.Soundex(4) \n",
    "print (soundex('ankit'))\n",
    "print (soundex('aunkit'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        4.2.3 Flexible String Matching\n",
    "        4.2.4 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'analytics': 1, 'on': 1, 'article': 1, 'an': 1, 'is': 1, 'This': 1, 'vidhya': 1})\n",
      "Counter({'analytics': 1, 'on': 1, 'language': 1, 'article': 1, 'natural': 1, 'is': 1, 'about': 1, 'processing': 1, 'vidhya': 1})\n",
      "0.629940788348712\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def get_cosine(vec1, vec2):\n",
    "    common = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "   \n",
    "    if not denominator:\n",
    "        return 0.0 \n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text): \n",
    "    words = text.split() \n",
    "    return Counter(words)\n",
    "\n",
    "text1 = 'This is an article on analytics vidhya' \n",
    "text2 = 'article on analytics vidhya is about natural language processing'\n",
    "\n",
    "vector1 = text_to_vector(text1) \n",
    "vector2 = text_to_vector(text2) \n",
    "cosine = get_cosine(vector1, vector2)\n",
    "print(vector1)\n",
    "print(vector2)\n",
    "print(cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
